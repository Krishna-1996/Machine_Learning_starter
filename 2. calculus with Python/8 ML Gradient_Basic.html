<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MACHINE LEARNING GRADIENT</title>
</head>
<body>
    <h1>Machine Learning Gradient</h1>

    <h2>1. Partial Derivative of Multi variant Function:</h2>
    <p>A partial derivative of a multi variable function measures how the function changes with respect to one of its variables while keeping the other variables constant. It essentially describes the rate of change of the function along a specific direction in its domain.</p>

    <h2>2. Partial Derivative Chain Rule:</h2>
    <p>This rule allows for the computation of partial derivatives of compositions of functions. It states that to find the derivative of a composition of functions, one must multiply the partial derivatives of the outer and inner functions.</p>

    <h2>3. Quadratic Cost:</h2>
    <p>Quadratic cost, also known as mean squared error (MSE), is a measure used in optimization problems, particularly in machine learning and statistics. It calculates the average of the squared differences between predicted and actual values, providing a measure of the overall discrepancy between the predicted and actual outcomes.</p>

    <h2>4. Gradients:</h2>
    <p>In calculus and vector calculus, a gradient represents the rate of change of a scalar field. It is a vector that points in the direction of the steepest ascent of the function at a given point and its magnitude represents the steepness of the ascent.</p>

    <h2>5. Gradient Descent:</h2>
    <p>Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of the negative gradient of the function. It is commonly used in machine learning for training models by adjusting parameters to minimize a cost function.</p>

    <h2>6. Back-Propagation:</h2>
    <p>Back-propagation is a fundamental algorithm used in training artificial neural networks. It computes the gradients of the cost function with respect to the weights of the network by applying the chain rule of calculus recursively from the output layer to the input layer. This allows for efficient adjustment of the network's parameters during the learning process.</p>
</body>
</html>
